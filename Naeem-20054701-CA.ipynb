{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e309bf",
   "metadata": {},
   "source": [
    "# B9AI105 REINFORCEMENT LEARNING - CA_ONE_(60%)\n",
    "\n",
    "Name: Naeem ul Hassan\n",
    "\n",
    "Student Number: 20054701\n",
    "\n",
    "Email: 20054701@mydbs.ie\n",
    "\n",
    "Course: B9AI105 REINFORCEMENT LEARNING (B9AI105_2425_TMD3)\n",
    "\n",
    "\n",
    "Assessment Type: CA_ONE_(60%)\n",
    "\n",
    "Assignment: Deep Reinforcement Learning Concepts Project\n",
    "\n",
    "Due: Monday, 14 July 2025, 11:59 PM\n",
    "\n",
    "Submission Date: 12 July 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6d3d5",
   "metadata": {},
   "source": [
    "# Custom HVAC Control with Double DQN and Policy‑Gradient + Baseline  \n",
    "A hands‑on companion notebook for the research report “**Comparative Study of Double Deep Q‑Network and Policy‑Gradient‑with‑Baseline Controllers for Building HVAC Management**”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81340e83",
   "metadata": {},
   "source": [
    "## 1  Environment Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce3b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "# We import all required libraries _once_ here so the rest of the notebook\n",
    "# remains tidy.  Everything below should run on Python 3.9+ with\n",
    "# PyTorch 2.x and Gymnasium 0.29+.\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "# This ensures that the random numbers generated are the same each time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "from __future__ import annotations   # enables `X | None` in 3.8/3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33fa44",
   "metadata": {},
   "source": [
    "## 2 HVAC Environment Definition \n",
    "\n",
    "Instead of using a complex EnergyPlus simulation (which can be slow and difficult to set up), we created a simplified thermal model that captures the essential physics of building heating and cooling.\n",
    "Our model represents a two-zone office building where:\n",
    "Physical Setup:\n",
    "\n",
    "Each zone (like an office room) stores heat like a thermal battery with capacity C\n",
    "Heat flows in and out through the walls and windows based on conductance U\n",
    "An air handling unit (AHU) supplies conditioned air to maintain comfort\n",
    "\n",
    "**Control Actions:**\n",
    "\n",
    "Discrete action: Set the supply air temperature (16°C to 22°C in 1°C steps)\n",
    "Continuous action: Adjust damper position (0-100%) to control airflow rate\n",
    "\n",
    "**Reward System:**\n",
    "The system tries to minimize two competing objectives:\n",
    "\n",
    "Energy cost: Electricity used by chillers and fans\n",
    "Comfort penalty: Applied when occupants feel too hot or cold (measured by PMV - Predicted Mean Vote)\n",
    "\n",
    "The goal is to keep people comfortable while using as little energy as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3511d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVACEnv(gym.Env):\n",
    "    \"\"\"A minimal two‑zone HVAC environment.\n",
    "\n",
    "    State (np.array, dtype=float32, shape=(10,)):\n",
    "        [0] outdoor_temp               (°C)\n",
    "        [1] outdoor_rh                 (%)\n",
    "        [2] zone0_temp                 (°C)\n",
    "        [3] zone0_rh                   (%)\n",
    "        [4] zone1_temp                 (°C)\n",
    "        [5] zone1_rh                   (%)\n",
    "        [6] occupancy_flag             {0,1}\n",
    "        [7] supply_air_temp_setpoint   (°C)\n",
    "        [8] damper_position            (0‑1)\n",
    "        [9] time_of_day                (sin encoded)\n",
    "\n",
    "    Actions (Tuple):\n",
    "        * Discrete(7): supply‑air temp ∈ {16,…,22} °C\n",
    "        * Box(low=0, high=1, shape=(1,)): damper (0=closed, 1=open)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, ep_minutes: int = 24 * 60, step_minutes: int = 15):\n",
    "        super().__init__()\n",
    "        self.ep_steps = ep_minutes // step_minutes\n",
    "        self.dt = step_minutes * 60  # seconds\n",
    "        \n",
    "        # --- action & observation spaces\n",
    "        self.action_space = spaces.Tuple(\n",
    "            (spaces.Discrete(7), spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32))\n",
    "        )\n",
    "        low_state = np.array([-40, 0, 0, 0, 0, 0, 0, 16, 0, -1], dtype=np.float32)\n",
    "        high_state = np.array([50, 100, 50, 100, 50, 100, 1, 22, 1, 1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low_state, high_state, dtype=np.float32)\n",
    "\n",
    "        # --- thermal parameters (realistic values)\n",
    "        self.C_zone = [1.5e6, 1.2e6]  # J/K for zones 0 and 1\n",
    "        self.U_facade = [220.0, 180.0]  # W/K for zones 0 and 1\n",
    "        self.zone_areas = [100.0, 80.0]  # m² for zones 0 and 1\n",
    "        self.internal_gains = [500.0, 300.0]  # W for zones 0 and 1\n",
    "        \n",
    "        self.C_p_air = 1012.0   # J/(kg·K)\n",
    "        self.m_flow_nom = 0.8   # kg/s @ damper=1\n",
    "        self.fan_power_coef = 0.8  # kW/(kg/s)^3 (cubic fan law)\n",
    "        self.chiller_COP = 3.0\n",
    "\n",
    "        self.outdoor_temp_profile = self._gen_outdoor_temp_profile()\n",
    "\n",
    "        self.state: np.ndarray | None = None\n",
    "        self._step_count = 0\n",
    "\n",
    "    def reset(self, *, seed: int = None, options: dict | None = None):\n",
    "        super().reset(seed=seed)\n",
    "        self._step_count = 0\n",
    "        \n",
    "        # Initialize zones near setpoint with small random variation\n",
    "        z0 = 22.0 + np.random.randn() * 0.5\n",
    "        z1 = 22.0 + np.random.randn() * 0.5\n",
    "        rh0 = rh1 = 50.0\n",
    "        occ = 1.0\n",
    "        sat = 18.0\n",
    "        damper = 0.5\n",
    "        tod = 0.0\n",
    "        \n",
    "        self.state = np.array(\n",
    "            [self.outdoor_temp_profile[0], 60.0, z0, rh0, z1, rh1, occ, sat, damper, tod], \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        return self.state.copy(), {}\n",
    "\n",
    "    def step(self, action: Tuple[int, np.ndarray]):\n",
    "        sat_choice, damper = action\n",
    "        damper = float(damper[0])\n",
    "        sat_temp = float(sat_choice + 16)  # Convert 0-6 to 16-22°C\n",
    "\n",
    "        # Unpack state\n",
    "        (outdoor_temp, outdoor_rh, z0_temp, z0_rh, z1_temp, z1_rh, \n",
    "         occ, _, _, tod) = self.state\n",
    "\n",
    "        # Simulate thermal response for each zone\n",
    "        m_dot = self.m_flow_nom * damper\n",
    "        delta_z0 = self._thermal_delta(z0_temp, outdoor_temp, sat_temp, m_dot, zone_id=0)\n",
    "        delta_z1 = self._thermal_delta(z1_temp, outdoor_temp, sat_temp, m_dot, zone_id=1)\n",
    "\n",
    "        z0_temp += delta_z0\n",
    "        z1_temp += delta_z1\n",
    "        \n",
    "        # Apply realistic temperature bounds\n",
    "        z0_temp = np.clip(z0_temp, 10.0, 40.0)\n",
    "        z1_temp = np.clip(z1_temp, 10.0, 40.0)\n",
    "\n",
    "        # Corrected energy calculations\n",
    "        avg_zone_temp = (z0_temp + z1_temp) / 2\n",
    "        temp_diff = max(0.0, avg_zone_temp - sat_temp)  # Cooling load\n",
    "        chiller_load_kw = m_dot * self.C_p_air * temp_diff / 1e3\n",
    "        chiller_energy_kwh = chiller_load_kw / self.chiller_COP * (self.dt / 3600)\n",
    "        fan_energy_kwh = (self.fan_power_coef * m_dot**3) * (self.dt / 3600)\n",
    "        total_energy = chiller_energy_kwh + fan_energy_kwh\n",
    "\n",
    "        # Improved comfort calculation\n",
    "        comfort_penalty = 0.0\n",
    "        for zone_temp in (z0_temp, z1_temp):\n",
    "            comfort_penalty += self._calculate_pmv_penalty(zone_temp)\n",
    "\n",
    "        # Balanced reward function\n",
    "        energy_penalty = total_energy * 100  # Scale energy appropriately\n",
    "        comfort_penalty_scaled = comfort_penalty * 10\n",
    "        reward = -(energy_penalty + comfort_penalty_scaled)\n",
    "\n",
    "        # Advance time\n",
    "        self._step_count += 1\n",
    "        done = (self._step_count >= self.ep_steps or \n",
    "                comfort_penalty > 10 or \n",
    "                any(temp < 15 or temp > 30 for temp in (z0_temp, z1_temp)))\n",
    "\n",
    "        # Update state\n",
    "        tod_angle = 2 * math.pi * (self._step_count / self.ep_steps)\n",
    "        outdoor_idx = self._step_count % len(self.outdoor_temp_profile)\n",
    "        \n",
    "        self.state = np.array([\n",
    "            self.outdoor_temp_profile[outdoor_idx],\n",
    "            outdoor_rh,\n",
    "            z0_temp,\n",
    "            z0_rh,\n",
    "            z1_temp,\n",
    "            z1_rh,\n",
    "            occ,\n",
    "            sat_temp,\n",
    "            damper,\n",
    "            math.sin(tod_angle),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        info = {\n",
    "            \"energy_kwh\": total_energy,\n",
    "            \"comfort_penalty\": comfort_penalty,\n",
    "            \"chiller_energy\": chiller_energy_kwh,\n",
    "            \"fan_energy\": fan_energy_kwh,\n",
    "            \"zone_temps\": [z0_temp, z1_temp],\n",
    "            \"outdoor_temp\": self.outdoor_temp_profile[outdoor_idx]\n",
    "        }\n",
    "\n",
    "        return self.state.copy(), reward, done, False, info\n",
    "\n",
    "    def _thermal_delta(self, zone_t, outdoor_t, supply_t, m_dot, zone_id=0):\n",
    "        \"\"\"Calculate temperature change for a zone\"\"\"\n",
    "        # Zone-specific parameters\n",
    "        zone_area = self.zone_areas[zone_id]\n",
    "        internal_gains = self.internal_gains[zone_id] if self.state[6] > 0 else 0\n",
    "        \n",
    "        # Heat transfer calculations\n",
    "        q_env = self.U_facade[zone_id] * (outdoor_t - zone_t)\n",
    "        q_supply = m_dot * self.C_p_air * (supply_t - zone_t) * 0.5  # Split airflow\n",
    "        q_internal = internal_gains\n",
    "        \n",
    "        # Temperature change\n",
    "        delta_T = (q_env + q_supply + q_internal) / self.C_zone[zone_id] * self.dt\n",
    "        return delta_T\n",
    "\n",
    "    def _calculate_pmv_penalty(self, zone_temp):\n",
    "        \"\"\"Calculate comfort penalty based on zone temperature\"\"\"\n",
    "        if 20 <= zone_temp <= 24:\n",
    "            return 0.0\n",
    "        elif zone_temp < 18 or zone_temp > 28:\n",
    "            return 10.0  # Severe discomfort\n",
    "        else:\n",
    "            return abs(zone_temp - 22) * 2.0  # Moderate discomfort\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_outdoor_temp_profile():\n",
    "        \"\"\"Generate realistic daily temperature profile\"\"\"\n",
    "        # 24-hour temperature profile: cooler at night, warmer during day\n",
    "        hours = np.linspace(0, 24, 96, endpoint=False)\n",
    "        base_temp = 20\n",
    "        daily_variation = 8 * np.sin(2 * np.pi * (hours - 6) / 24)  # Peak at 2 PM\n",
    "        return base_temp + daily_variation\n",
    "\n",
    "    def render(self):\n",
    "        if self.state is not None:\n",
    "            print(f\"Step {self._step_count}: \"\n",
    "                  f\"Zone0={self.state[2]:.1f}°C, Zone1={self.state[4]:.1f}°C, \"\n",
    "                  f\"Outdoor={self.state[0]:.1f}°C, SAT={self.state[7]:.1f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593701b",
   "metadata": {},
   "source": [
    "## 3  Utility Classes & Helpers\n",
    "Below we implement a generic **ReplayBuffer** and some tiny neural‑network\n",
    "builder helpers to keep the agent classes compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3638eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Transition:\n",
    "    state: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size cyclic buffer for off-policy algorithms (Double DQN).\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 50_000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state: np.ndarray, action: np.ndarray, reward: float, \n",
    "             next_state: np.ndarray, done: bool):\n",
    "        \"\"\"Add a transition to the buffer.\"\"\"\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int = 64) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            raise ValueError(f\"Buffer size {len(self.buffer)} < batch_size {batch_size}\")\n",
    "        \n",
    "        batch = random.sample(list(self.buffer), batch_size)\n",
    "        states = torch.tensor(np.stack([t.state for t in batch]), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.stack([t.action for t in batch]), dtype=torch.int64)\n",
    "        rewards = torch.tensor([t.reward for t in batch], dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.stack([t.next_state for t in batch]), dtype=torch.float32)\n",
    "        dones = torch.tensor([t.done for t in batch], dtype=torch.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def mlp(input_dim: int, output_dim: int, hidden: int = 128) -> nn.Sequential:\n",
    "    \"\"\"Return a 3-layer MLP with ReLU activations.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, output_dim),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877f601",
   "metadata": {},
   "source": [
    "## 4  Double DQN Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42ce4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "    \"\"\"Double Deep Q‑Network implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        gamma: float = 0.995,\n",
    "        lr: float = 1e-3,\n",
    "        tau: float = 0.005,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.05,\n",
    "        epsilon_decay: int = 20_000,\n",
    "        buffer_capacity: int = 50_000,\n",
    "        batch_size: int = 64,\n",
    "        device: str | torch.device = \"cpu\",\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        # Discrete branch only (supply‑temp); continuous action sampled separately\n",
    "        act_dim = env.action_space[0].n\n",
    "\n",
    "        self.online_net = mlp(obs_dim, act_dim).to(self.device)\n",
    "        self.target_net = mlp(obs_dim, act_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=lr)\n",
    "        self.replay = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "        # Epsilon‑greedy parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.steps_done = 0\n",
    "\n",
    "    # ---------------------- interaction & learning --------------------- #\n",
    "    def select_action(self, state: np.ndarray, damper_cont: float):\n",
    "        \"\"\"Return (discrete_supply_temp, damper_position).\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            temp_action = self.env.action_space[0].sample()\n",
    "        else:\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.online_net(state_t)\n",
    "            temp_action = int(torch.argmax(q_values).item())\n",
    "        return temp_action, np.array([damper_cont], dtype=np.float32)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_end, self.epsilon - (1.0 - self.epsilon_end) / self.epsilon_decay\n",
    "        )\n",
    "\n",
    "    def soft_update(self):\n",
    "        for tgt, src in zip(self.target_net.parameters(), self.online_net.parameters()):\n",
    "            tgt.data.copy_(tgt.data * (1.0 - self.tau) + src.data * self.tau)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        actions = actions[:, 0].unsqueeze(1).to(self.device)  # discrete branch\n",
    "        rewards = rewards.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        # Compute Double DQN targets\n",
    "        next_q_online = self.online_net(next_states)\n",
    "        next_actions = torch.argmax(next_q_online, dim=1, keepdim=True)\n",
    "        next_q_target = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        targets = rewards + self.gamma * next_q_target * (1.0 - dones)\n",
    "\n",
    "        q_vals = self.online_net(states).gather(1, actions).squeeze(1)\n",
    "        loss = F.mse_loss(q_vals, targets.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update()\n",
    "\n",
    "    # ----------------------------- training ---------------------------- #\n",
    "    def train(self, episodes: int = 50):\n",
    "        reward_history = []\n",
    "        for ep in range(episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            ep_reward = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Damper continuous action sampled from simple heuristic\n",
    "                damper = 0.7  # could also be random or learned separately\n",
    "                action = self.select_action(state, damper)\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "                self.replay.push(state, np.array(action), reward, next_state, done)\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "\n",
    "                self.learn()\n",
    "                self.update_epsilon()\n",
    "                self.steps_done += 1\n",
    "\n",
    "            reward_history.append(ep_reward)\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"Episode {ep:03d} | Reward: {ep_reward:8.2f} | ε={self.epsilon:.3f}\")\n",
    "\n",
    "        return reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4776b",
   "metadata": {},
   "source": [
    "## 5  Policy‑Gradient + Baseline Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd743bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientBaselineAgent:\n",
    "    \"\"\"REINFORCE with value baseline (a tiny actor‑critic).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 3e-4,\n",
    "        entropy_coef: float = 0.01,\n",
    "        device: str | torch.device = \"cpu\",\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space[0].n  # discrete temp setpoints\n",
    "\n",
    "        self.actor = mlp(obs_dim, act_dim).to(self.device)\n",
    "        self.critic = mlp(obs_dim, 1).to(self.device)\n",
    "\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "    # -------------------------- utils -------------------------- #\n",
    "    def _compute_returns(self, rewards: List[float]) -> torch.Tensor:\n",
    "        R = 0.0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    # ----------------------- training loop --------------------- #\n",
    "    def train(self, episodes: int = 50, batch_size: int = 5):\n",
    "        ep_rewards = []\n",
    "        all_lengths = []\n",
    "        batch_states, batch_actions, batch_log_probs, batch_rewards = [], [], [], []\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            ep_reward = 0.0\n",
    "            while not done:\n",
    "                # Damper continuous action follows stochastic Beta(2,2) for exploration\n",
    "                damper = np.random.beta(2, 2)\n",
    "                state_t = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "                logits = self.actor(state_t)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                temp_action = dist.sample()\n",
    "                log_prob = dist.log_prob(temp_action)\n",
    "\n",
    "                action = (int(temp_action.item()) + 16, np.array([damper], dtype=np.float32))\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "\n",
    "                batch_states.append(state_t)\n",
    "                batch_actions.append(temp_action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                batch_rewards.append(reward)\n",
    "\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "\n",
    "            ep_rewards.append(ep_reward)\n",
    "            all_lengths.append(len(batch_rewards))\n",
    "\n",
    "            # Update after every `batch_size` episodes\n",
    "            if (ep + 1) % batch_size == 0:\n",
    "                self._update_policy(batch_states, batch_actions, batch_log_probs, batch_rewards)\n",
    "                batch_states, batch_actions, batch_log_probs, batch_rewards = [], [], [], []\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"Episode {ep:03d} | Reward: {ep_reward:8.2f}\")\n",
    "\n",
    "        return ep_rewards\n",
    "\n",
    "    # ----------------------- policy update --------------------- #\n",
    "    def _update_policy(self, states, actions, log_probs, rewards):\n",
    "        returns = self._compute_returns(rewards)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        log_probs = torch.stack(log_probs).to(self.device)\n",
    "        states_tensor = torch.stack(states).to(self.device)\n",
    "        actions_tensor = torch.stack(actions).to(self.device)\n",
    "\n",
    "        # Critic loss\n",
    "        values = self.critic(states_tensor).squeeze(1)\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "\n",
    "        # Actor loss (with baseline)\n",
    "        advantages = returns - values.detach()\n",
    "        actor_loss = -(log_probs * advantages).mean() - self.entropy_coef * torch.distributions.Categorical(logits=self.actor(states_tensor)).entropy().mean()\n",
    "\n",
    "        # backward\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7f83f",
   "metadata": {},
   "source": [
    "## 6  Training of our models\n",
    "We will train each agent for **50 episodes** (≈ 50 days) which is enough\n",
    "to illustrate learning curves yet short enough to run on CPU in under\n",
    "5 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b34706d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎮 Training Double DQN ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pg_agent \u001b[38;5;241m=\u001b[39m PolicyGradientBaselineAgent(env_pg, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎮 Training Double DQN ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m dqn_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎮 Training Policy‑Gradient + Baseline ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m pg_rewards \u001b[38;5;241m=\u001b[39m pg_agent\u001b[38;5;241m.\u001b[39mtrain(episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 98\u001b[0m, in \u001b[0;36mDoubleDQNAgent.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     96\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state, damper)\n\u001b[1;32m     97\u001b[0m next_state, reward, done, _, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay\u001b[38;5;241m.\u001b[39mpush(state, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m, reward, next_state, done)\n\u001b[1;32m     99\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    100\u001b[0m ep_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "env_dqn = HVACEnv()\n",
    "env_pg = HVACEnv()\n",
    "\n",
    "dqn_agent = DoubleDQNAgent(env_dqn, device=\"cpu\")\n",
    "pg_agent = PolicyGradientBaselineAgent(env_pg, device=\"cpu\")\n",
    "\n",
    "print(\"\\n🎮 Training Double DQN ...\")\n",
    "dqn_rewards = dqn_agent.train(episodes=50)\n",
    "\n",
    "print(\"\\n🎮 Training Policy‑Gradient + Baseline ...\")\n",
    "pg_rewards = pg_agent.train(episodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bf4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
